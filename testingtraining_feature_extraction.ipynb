{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = ''\n",
    "train_path = os.path.join(root_path, 'train')\n",
    "\n",
    "os.makedirs(train_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "Extracts important features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9377/9377 [00:30<00:00, 305.50it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train/testing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m np\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_names.npy\u001b[39m\u001b[38;5;124m'\u001b[39m), class_names)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# converts test data to numpy arrays\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m test_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_files)\n\u001b[1;32m     39\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((n, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m244\u001b[39m, \u001b[38;5;241m244\u001b[39m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train/testing'"
     ]
    }
   ],
   "source": [
    "train_data = os.path.join(train_path, 'training')\n",
    "test_data = os.path.join(train_path, 'testing')\n",
    "\n",
    "dst = os.path.join(train_path, 'dimension_reduced_data')\n",
    "\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "initial_transforms = transforms.Compose([\n",
    "    transforms.Resize((244, 244)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(train_data, transform=initial_transforms)\n",
    "\n",
    "# converts training data tensors to numpy arrays\n",
    "n = len(dataset)\n",
    "X = np.zeros((n, 3, 244, 244))\n",
    "y = np.zeros(n)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(tqdm(dataset)):\n",
    "    X[i] = inputs.numpy()\n",
    "    y[i] = labels\n",
    "\n",
    "# class to index dictionary\n",
    "class_to_idx = dataset.class_to_idx\n",
    "with open(os.path.join(dst, 'class_to_idx.pkl'), 'wb') as handle:\n",
    "    pickle.dump(class_to_idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# class names numpy array\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "np.save(os.path.join(dst, 'class_names.npy'), class_names)\n",
    "\n",
    "\n",
    "# converts test data to numpy arrays\n",
    "test_files = os.listdir(test_data)\n",
    "\n",
    "n = len(test_files)\n",
    "X_test = np.zeros((n, 3, 244, 244))\n",
    "\n",
    "for file in os.listdir(test_data):\n",
    "    img = Image.open(f'{test_data}/{file}')\n",
    "    X_test[i] = np.asanyarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# normalize training data\n",
    "n, d1, d2, d3 = X.shape\n",
    "X = X.reshape((n, d1 * d2 * d3))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# normalize testing data\n",
    "n, d1, d2, d3 = X_test.shape\n",
    "X_test = X_test.reshape((n, d1 * d2 * d3))\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Dimension reduction on data for full rank matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(\u001b[38;5;241m0.90\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(0.90)\n",
    "\n",
    "X = pca.fit_transform(X)\n",
    "X_test = pca.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "Supervised dimension reduction on data that will be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "X = lda.fit_transform(X, y)\n",
    "X_test = lda.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves data\n",
    "- training data: [dimension_reduced_data/X.npy](dimension_reduced_data/X_train_pca_lda.npy)\n",
    "- labels: [dimension_reduced_data/y.npy](dimension_reduced_data/y.npy)\n",
    "- testing data: [dimension_reduced_data/X_test.npy](dimension_reduced_data/X_test.npy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(dst, 'X.npy'), X)\n",
    "np.save(os.path.join(dst, 'y.npy'), y)\n",
    "\n",
    "np.save(os.path.join(dst, 'X_test.npy'), X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
